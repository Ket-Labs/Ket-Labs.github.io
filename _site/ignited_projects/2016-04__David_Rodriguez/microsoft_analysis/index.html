<!DOCTYPE html>
<html>


<link rel="shortcut icon" type="image/png" href="/images/favicon.png">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Sentiment and Principal Component Analysis of Twitter Data: Microsoft Study</title>
  <meta name="description" content="Get a data science upgrade for your team, research lab or personal project. 
">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="http://www.ket-labs.com/ignited_projects/2016-04__David_Rodriguez/microsoft_analysis/">
  <link rel="alternate" type="application/rss+xml" title="Ket Labs" href="http://www.ket-labs.com/feed.xml" />
<link rel='stylesheet' id='open-sans-css'  href='//fonts.googleapis.com/css?family=Open+Sans%3A300italic%2C400italic%2C600italic%2C300%2C400%2C600&#038;subset=latin%2Clatin-ext&#038;ver=4.2.4' type='text/css' media='all' />
<link href='http://fonts.googleapis.com/css?family=Titillium+Web:600italic,600,400,400italic' rel='stylesheet' type='text/css'>





<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-77268505-1', 'auto');
  ga('send', 'pageview');

</script>
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">Ket Labs</a>


    <nav class="site-nav">

      <a href="#" class="menu-icon menu.open">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>  

    <div class="trigger"><h1>Main Navigation</h1>

 <ul class="menu">

    
    
     <li><a href="/data_science/" class="page-link">Data Science</a>
    
    </li>
    
    
     <li><a href="/who_we_are/" class="page-link">Who we are</a>
    
    </li>
    
    
    <li><a href="/bootcamp/" class="page-link">Bootcamp</a>
    <ul class="sub-menu">
    
    <li><a href="/bootcamp/">Bootcamp</a></li>
    
    <li><a href="/bootcamp/bootcamp_apply/">Apply for Bootcamp</a></li>
    
    </ul>
    
    </li>
    
    
     <li><a href="/ignited_projects/" class="page-link">Ignited Projects</a>
    
    </li>
    
    
     <li><a href="/contact/" class="page-link">Contact</a>
    
    </li>
    
    </ul>


<!-- <ul class="menu">
        <li> <a class="page-link" href="/about">About</a></li>
        <li> <a class="page-link"  href="/blog">Blog</a>
        <li> <a class="page-link" href="/blog">CV</a>
        <li> <a class="page-link" href="/blog">For Students</a></li>
        <li> <a class="page-link"  href="/blog">Research</a></a>
        <li> <a class="page-link" href="/blog">Teaching</a>
<ul class="sub-menu">
	<li><a href="http://svmiller.com/teaching/posc-1020-introduction-to-international-relations/">POSC 1020 – Introduction to International Relations</a></li>
	<li><a href="http://svmiller.com/teaching/posc-3410-quantitative-methods-in-political-science/">POSC 3410 – Quantitative Methods in Political Science</a></li>
	<li><a href="http://svmiller.com/teaching/posc-3610-international-politics-in-crisis/">POSC 3610 – International Politics in Crisis</a></li>
	<li><a href="http://svmiller.com/teaching/posc-3630-united-states-foreign-policy/">POSC 3630 – United States Foreign Policy</a></li>
</ul></li>
        <li> <a class="page-link" href="/blog">Miscellany</a>
<ul class="sub-menu">
	<li><a href="http://svmiller.com/teaching/posc-1020-introduction-to-international-relations/">Clean USAID Greenbook Data</a></li>
	<li><a href="http://svmiller.com/teaching/posc-3410-quantitative-methods-in-political-science/">Journal of Peace Research *.bst File</a></li>
	<li><a href="http://svmiller.com/teaching/posc-3610-international-politics-in-crisis/">My Custom Beamer Style</a></li>
</ul> 

</li>
</ul> -->

     </div>  
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">Sentiment and Principal Component Analysis of Twitter Data: Microsoft Study</h1>
  </header>

  <article class="post-content">
    <h2 id="introduction">Introduction</h2>

<p>Twitter is a powerful tool that enables users to communicate with others
and also empowers data scientists with large quantities of data they can
use. Communications on twitter are live and dynamic, changing every
second. They are also short, limited to 140 characters. The analysis of
such short, fast-moving data can reveal how people communicate on a
particular subject.</p>

<p>For example, by mining Twitter search data one can explore trends of
stock prices against sentiments or specific words. One can also explore
what makes a popular tweet, an excersize that can lead to efficient
social media marketing. One can also compare tweets for different search
terms, such as for different companies, and identfy what makes people
excited about them or what their more prominent complaints are.</p>

<p>For this analysis, I have chosen to gather recent English-language
tweets containing the word ‘microsoft’. This allows me to explore how
people regard the microsoft brand, as well as how they treat any news
articles referring to microsoft. The end goal of my study was to attempt
to <strong>predict how users would tweet about the specified subject</strong> based
on user information publicly available for Twitter accounts.</p>

<h2 id="experiment-setup">Experiment Setup</h2>

<p>Load up required packages:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>library(ggplot2)
library(twitteR)
library(tm)
library(rjson)
library(wordcloud)
library(dplyr)
library(caret)
library(knitr)
library(RColorBrewer)
library(stringr)
library(syuzhet) # for sentiment analysis
library(rattle)
library(lubridate)
library(rpart)
library(randomForest)
library(glmnet)
</code></pre>
</div>

<p>To access twitter, I need to provide authorization credentials for my
Twitter application:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>secrets &lt;- fromJSON(file='twitter_secrets.json.nogit')

setup_twitter_oauth(secrets$api_key,
                    secrets$api_secret,
                    secrets$access_token,
                    secrets$access_token_secret)

## [1] "Using direct authentication"
</code></pre>
</div>

<p>Perform a twitter search and extract the information I want:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>searchstring &lt;- 'microsoft'
numtweets &lt;- 10000
st &lt;- searchTwitter(searchstring, n=numtweets, resultType = 'recent', lang = 'en')

statuses &lt;- data.frame(text=sapply(st, function(x) x$getText()),
                       user=sapply(st, function(x) x$getScreenName()),
                       RT=sapply(st, function(x) x$isRetweet),
                       latitude=sapply(st, function(x) as.numeric(x$latitude[1])),
                       longitude=sapply(st, function(x) as.numeric(x$longitude[1])),
                       time=sapply(st, function(x) format(x$created, format='%F %T'))
                       )
</code></pre>
</div>

<p>Remove retweets for clarity:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>statuses &lt;-
    statuses %&gt;%
    filter(!RT)
</code></pre>
</div>

<p>Save tweets for future use:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>today &lt;- format(Sys.time(), '%Y-%m-%d')
savename &lt;- paste0('data/tweets_',searchstring,'_',
                   nrow(statuses),'_',today,'.Rda')
saveRDS(statuses, file=savename)
</code></pre>
</div>

<p>Alternatively, I load up prior searches to avoid re-running:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>files &lt;- list.files('data','tweets_')
searchstring &lt;- 'microsoft'
rm(statuses)

## Warning in rm(statuses): object 'statuses' not found

for(i in 1:length(files)) {
    selectedfile &lt;- paste0('data/',files[i])
    if(!exists('statuses')){
        statuses &lt;- readRDS(file=selectedfile)
    }else{
        statuses &lt;- rbind(statuses, readRDS(file=selectedfile))
    }
}
</code></pre>
</div>

<p>Total number of tweets to process is 13132</p>

<h2 id="text-analysis">Text Analysis</h2>

<p>Gather the tweets:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>textdata &lt;- Corpus(VectorSource(statuses$text))

textdata &lt;- 
    textdata %&gt;%
    tm_map(removeWords, stopwords("english"), mc.cores=1) %&gt;%
    tm_map(removePunctuation, mc.cores=1) %&gt;%
    tm_map(content_transformer(function(x) iconv(x, to='UTF-8-MAC', sub='byte')),
           mc.cores=1) %&gt;%
    tm_map(content_transformer(tolower), mc.cores=1) %&gt;%
    tm_map(content_transformer(function(x) str_replace_all(x, "@\\w+", "")), 
           mc.cores=1) %&gt;% # remove twitter handles
    tm_map(removeNumbers, mc.cores=1) %&gt;%
    tm_map(stemDocument, mc.cores=1) %&gt;%
    tm_map(stripWhitespace, mc.cores=1)

save(textdata, file = 'data/testdata_corpus.RData') 
rm(textdata)

load('data/testdata_corpus.RData') 
</code></pre>
</div>

<p>A quick wordcloud of the tweets reveals the 100 most commonly used
words:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>pal2 &lt;- brewer.pal(8,"RdBu")
wordcloud(textdata, max.words = 100, colors= pal2, random.order=F, 
          rot.per=0.1, use.r.layout=F)
</code></pre>
</div>

<p><img src="/ignited_projects/2016-04__David_Rodriguez/microsoft_analysis_files/figure-markdown_strict/wordcloud-1.png" alt="" /></p>

<p>I perform a sentiment analysis on the text data by comparing the words
with those from the <a href="http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm">NRC Word-Emotion Association
Lexicon</a>,
which assigns them to 8 emotions (eg, anger, joy, etc) and 2 sentiments
(postive and negative). I create a new variable, positivity, which is
the difference between the positive and negative sentiments.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>sentiments &lt;- sapply(textdata, function(x) get_nrc_sentiment(as.character(x)))

sentiments &lt;- as.data.frame(aperm(sentiments)) # transpose and save as dataframe
sentiments &lt;- as.data.frame(lapply(sentiments, as.numeric)) # a bit more to organize
sentiments &lt;-
    sentiments %&gt;%
    mutate(positivity = positive - negative)
</code></pre>
</div>

<p>Here are the emotions expressed in these tweets:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>emotions &lt;- data.frame("count"=colSums(sentiments[,c(1:8)]))
emotions &lt;- cbind("sentiment" = rownames(emotions), emotions)

ggplot(data = emotions, aes(x = sentiment, y = count)) +
    geom_bar(aes(fill = sentiment), stat = "identity") +
    xlab("Sentiment") + ylab("Total Count") + 
    scale_fill_brewer(palette='RdBu') + 
    theme_bw() + theme(legend.position='none')
</code></pre>
</div>

<p><img src="/ignited_projects/2016-04__David_Rodriguez/microsoft_analysis_files/figure-markdown_strict/sentiment_plot-1.png" alt="" /></p>

<p>Further processing to get word counts:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>dtm &lt;- DocumentTermMatrix(textdata)
dtm &lt;- inspect(dtm)

save(dtm, file = 'data/DocumentTermMatrix.RData') 
rm(dtm)

load('data/DocumentTermMatrix.RData') 
</code></pre>
</div>

<p>Sort in descending order to find the most common terms:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>words &lt;- data.frame(term = colnames(dtm))
words$count &lt;- colSums(dtm)

words &lt;-
    words %&gt;%
    arrange(desc(count))
head(words)

##        term count
## 1 microsoft 11856
## 2    window  2311
## 3      xbox  2255
## 4   hololen  1721
## 5      game  1599
## 6       new  1192
</code></pre>
</div>

<p>Convert tweets to data frame and select only the top 100 words to
process:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>tweets &lt;- as.data.frame(dtm)
ind &lt;- data.frame('id'=seq.int(nrow(tweets)))
tweets &lt;- cbind(ind, tweets)

words_100 &lt;- as.character(words[2:101,'term'])
tweets &lt;- tweets[,c('id',words_100)]

save(tweets, file = 'data/tweets.RData') 
rm(tweets, dtm)

rm(dtm, textdata)

## Warning in rm(dtm, textdata): object 'dtm' not found

load('data/tweets.RData') 
</code></pre>
</div>

<h2 id="principal-component-analysis">Principal Component Analysis</h2>

<p>I’ll now perform a principal component analysis on the tweet data set
and join the first 5 components to the original status array. This will
allow me to reduce the number of paramaters to consider. That is, rather
than considering each word individually, I can consider the linear
combination of all words with appropriate loading factors that
effectively group them by phrases.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>trans &lt;- preProcess(tweets[,2:ncol(tweets)], method=c("pca"), thresh = 0.95)
pca &lt;- predict(trans, tweets[,2:ncol(tweets)])
statuses &lt;- cbind(statuses, pca[,1:5], sentiments)
</code></pre>
</div>

<p>Let’s examine the reprojected data:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>pal2 &lt;- brewer.pal(10,"RdBu")
ggplot(statuses, aes(x=PC1, y=PC2)) + 
    geom_point(aes(fill=positivity), size=4, alpha=0.7, pch=21, stroke=1.3) + 
    scale_fill_gradientn(colours = pal2, limits=c(-5,5)) + theme_bw()
</code></pre>
</div>

<p><img src="/ignited_projects/2016-04__David_Rodriguez/microsoft_analysis_files/figure-markdown_strict/pca1-2_plot-1.png" alt="" /></p>

<p>Sometimes the other principal components are more illustrative:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>pal2 &lt;- brewer.pal(10,"RdBu")
ggplot(statuses, aes(x=PC2, y=PC3)) + 
    geom_point(aes(fill=positivity), size=4, alpha=0.7, pch=21, stroke=1.3) + 
    scale_fill_gradientn(colours = pal2, limits=c(-5,5)) + theme_bw()
</code></pre>
</div>

<p><img src="/ignited_projects/2016-04__David_Rodriguez/microsoft_analysis_files/figure-markdown_strict/pca2-3_plot-1.png" alt="" /></p>

<p>I remove outliers, which I define as the 2% highest and lowest PC values
in terms of PC1 and PC2:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>cutlevel &lt;- 2/100.
cut1 &lt;- quantile(statuses$PC1, probs=c(cutlevel,1-cutlevel))
cut2 &lt;- quantile(statuses$PC2, probs=c(cutlevel,1-cutlevel))

statuses &lt;- 
    statuses %&gt;%
    filter(PC1&gt;cut1[1] &amp; PC1&lt;cut1[2]) %&gt;%
    filter(PC2&gt;cut2[1] &amp; PC2&lt;cut2[2])
</code></pre>
</div>

<p>The loading factors reveal how important each term is to the principal
component axes. Here are the top few terms of the first two components:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>loadings &lt;- trans$rotation 
load_sqr &lt;- loadings^2

load_sqr &lt;- data.frame(load_sqr)
temp &lt;- data.frame('term'=rownames(load_sqr))
load_sqr &lt;- cbind(temp, load_sqr)
load_sqr %&gt;%
    select(term, PC1) %&gt;%
    arrange(desc(PC1)) %&gt;%
    head(10) %&gt;% kable(format='html')
</code></pre>
</div>

<table>
<thead>
<tr>
<th style="text-align:left;">
term
</th>
<th style="text-align:right;">
PC1
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
restor
</td>
<td style="text-align:right;">
0.0983066
</td>
</tr>
<tr>
<td style="text-align:left;">
slam
</td>
<td style="text-align:right;">
0.0977344
</td>
</tr>
<tr>
<td style="text-align:left;">
epic
</td>
<td style="text-align:right;">
0.0971905
</td>
</tr>
<tr>
<td style="text-align:left;">
faith
</td>
<td style="text-align:right;">
0.0969293
</td>
</tr>
<tr>
<td style="text-align:left;">
human
</td>
<td style="text-align:right;">
0.0939914
</td>
</tr>
<tr>
<td style="text-align:left;">
ballmer
</td>
<td style="text-align:right;">
0.0914452
</td>
</tr>
<tr>
<td style="text-align:left;">
steve
</td>
<td style="text-align:right;">
0.0911956
</td>
</tr>
<tr>
<td style="text-align:left;">
dunk
</td>
<td style="text-align:right;">
0.0891933
</td>
</tr>
<tr>
<td style="text-align:left;">
former
</td>
<td style="text-align:right;">
0.0891634
</td>
</tr>
<tr>
<td style="text-align:left;">
ceo
</td>
<td style="text-align:right;">
0.0869296
</td>
</tr>
</tbody>
</table>
<div class="highlighter-rouge"><pre class="highlight"><code>load_sqr %&gt;%
    select(term, PC2) %&gt;%
    arrange(desc(PC2)) %&gt;%
    head(10) %&gt;% kable(format='html')
</code></pre>
</div>

<table>
<thead>
<tr>
<th style="text-align:left;">
term
</th>
<th style="text-align:right;">
PC2
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
fun
</td>
<td style="text-align:right;">
0.1589405
</td>
</tr>
<tr>
<td style="text-align:left;">
csrrace
</td>
<td style="text-align:right;">
0.1557036
</td>
</tr>
<tr>
<td style="text-align:left;">
have
</td>
<td style="text-align:right;">
0.1487819
</td>
</tr>
<tr>
<td style="text-align:left;">
join
</td>
<td style="text-align:right;">
0.1443763
</td>
</tr>
<tr>
<td style="text-align:left;">
play
</td>
<td style="text-align:right;">
0.1277830
</td>
</tr>
<tr>
<td style="text-align:left;">
free
</td>
<td style="text-align:right;">
0.1087408
</td>
</tr>
<tr>
<td style="text-align:left;">
window
</td>
<td style="text-align:right;">
0.0359545
</td>
</tr>
<tr>
<td style="text-align:left;">
hololen
</td>
<td style="text-align:right;">
0.0129898
</td>
</tr>
<tr>
<td style="text-align:left;">
develop
</td>
<td style="text-align:right;">
0.0115247
</td>
</tr>
<tr>
<td style="text-align:left;">
preorder
</td>
<td style="text-align:right;">
0.0098478
</td>
</tr>
</tbody>
</table>
<p>I’ve created a function to sample tweets across the PC spectrum.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>set.seed(42)
tweet_check &lt;- function(text, pc, numbreaks=5){
    cuts &lt;- cut(pc, numbreaks)
    #cuts &lt;- cut(pc, breaks=quantile(pc, probs=seq(0,1,1/numbreaks)))
    temp &lt;- data.frame(text=text, pc=pc, pc_val=cuts)
    temp &lt;- temp %&gt;%
        group_by(pc_val) %&gt;%
        summarise(text=iconv(sample(text,1), to='UTF-8-MAC', sub='byte')) %&gt;%
        filter(!is.na(pc_val))
    return(temp)
}
</code></pre>
</div>

<p>After examining some of the results, I found an interesting trend for
PC2:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>set.seed(42)
tweet_check(statuses$text, statuses$PC2, 10) %&gt;% kable(format='html')
</code></pre>
</div>

<table>
<thead>
<tr>
<th style="text-align:left;">
pc\_val
</th>
<th style="text-align:left;">
text
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
(-4.36,-3.67\]
</td>
<td style="text-align:left;">
You won't need an Xbox to play the next generation of Microsoft games -
CNET: Microsoft will bring its games… &lt;https://t.co/wcXr4cbSDm&gt; |Cnet
</td>
</tr>
<tr>
<td style="text-align:left;">
(-3.67,-2.99\]
</td>
<td style="text-align:left;">
You won't need an Xbox to play Microsoft's next generation of games
&lt;https://t.co/BjA8WogLe5&gt;
</td>
</tr>
<tr>
<td style="text-align:left;">
(-2.99,-2.3\]
</td>
<td style="text-align:left;">
WORLD | You won't need an Xbox to play Microsoft's next generation of
games | Read: &lt;https://t.co/iRMVb4pf52&gt; via Yahoo!© News
</td>
</tr>
<tr>
<td style="text-align:left;">
(-2.3,-1.62\]
</td>
<td style="text-align:left;">
@MoiMarkus Have you tried Office Lens? &lt;https://t.co/9Cydi5kP77&gt;
</td>
</tr>
<tr>
<td style="text-align:left;">
(-1.62,-0.933\]
</td>
<td style="text-align:left;">
Telstra offers 200GB free Microsoft OneDrive storage
&lt;https://t.co/DuGr5ig3mf&gt;
</td>
</tr>
<tr>
<td style="text-align:left;">
(-0.933,-0.248\]
</td>
<td style="text-align:left;">
AppOfTheDay:"Voice Record+", a \#WindowsPhone \#App NOW FREE! on
\#AppDeals &gt;&gt; &lt;https://t.co/TWV12YmN6I&gt; &lt;https://t.co/5C4cstEpaw&gt;
</td>
</tr>
<tr>
<td style="text-align:left;">
(-0.248,0.436\]
</td>
<td style="text-align:left;">
Measuring and evaluating visual desirability; how to measure visual
appeal of a design or website: &lt;https://t.co/5WzVykYjEH&gt;
</td>
</tr>
<tr>
<td style="text-align:left;">
(0.436,1.12\]
</td>
<td style="text-align:left;">
Microsoft Beijing Accelerator 7th Batch Demo Day - BEIJING, March
BEIJING, March 1, 2016 /PRNewswire/ .. &lt;https://t.co/pVugdeujWa&gt;
\#microsoft
</td>
</tr>
<tr>
<td style="text-align:left;">
(1.12,1.8\]
</td>
<td style="text-align:left;">
Microsoft's Hololens is up for pre-order, here's hoping you can expense
it &lt;https://t.co/00w3rl7R8R&gt;
</td>
</tr>
<tr>
<td style="text-align:left;">
(1.8,2.5\]
</td>
<td style="text-align:left;">
@zippylab Microsoft announces HoloLens specs, preorder dates, and what's
in the Developer Edition &lt;https://t.co/3fTvtH44ko&gt; via neowin
</td>
</tr>
</tbody>
</table>
<p>I notice that low PC2 values tend to be about the Xbox and high PC2
values tend to be about the Hololens. This presents a possible avenue
for the predictive analysis. If I can devise a model to predict the
value of PC2 based on a Twitter user’s data, I’ll know which product
they are more excited about.</p>

<h2 id="gather-user-data">Gather User Data</h2>

<p>To carry out my models, I’ll need a lot more information. I gather all
the user data for each particular tweet.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>userlist &lt;- sapply(unique(statuses$user), as.character)
allusers &lt;- lookupUsers(userlist)

# Gather all the user info in a data frame
userinfo &lt;- data.frame(user=sapply(allusers, function(x) x$screenName),
                       realname=sapply(allusers, function(x) x$name),
                       numstatuses=sapply(allusers, function(x) x$statusesCount),
                       followers=sapply(allusers, function(x) x$followersCount),
                       friends=sapply(allusers, function(x) x$friendsCount),
                       favorites=sapply(allusers, function(x) x$favoritesCount),
                       account_created=sapply(allusers, function(x) format(x$created, 
                                                              format='%F %T')),
                       verified=sapply(allusers, function(x) x$verified),
                       numlists=sapply(allusers, function(x) x$listedCount)) %&gt;%
    mutate(user=as.character(user)) %&gt;%
    mutate(twitter_years=interval(account_created, Sys.time()) / dyears(1)) %&gt;%
    select(-account_created)

save(userinfo, file='data/userinfo.Rdata')
#rm(userinfo)

load('data/userinfo.Rdata')
</code></pre>
</div>

<p>The original tweets will now be grouped together by user (taking
averages of the relevant quantities of interest) and then joined
together to the user information data frame:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>newstatuses &lt;-
    statuses %&gt;%
    group_by(user) %&gt;%
    summarize(numTopicTweets=n(),
              positivity=mean(positivity),
              PC1=mean(PC1),
              PC2=mean(PC2),
              PC3=mean(PC3),
              PC4=mean(PC4),
              PC5=mean(PC5),
              client=rownames(sort(table(client), decreasing = T))[1],
              anger=mean(anger), anticipation=mean(anticipation), 
              disgust=mean(disgust), fear=mean(fear), joy=mean(joy),
              sadness=mean(sadness), surprise=mean(surprise), trust=mean(trust)) %&gt;% 
    mutate(user=as.character(user))

# Join the data together
alldata &lt;- inner_join(userinfo, newstatuses, by='user')
</code></pre>
</div>

<h2 id="predictive-analysis">Predictive Analysis</h2>

<h3 id="setup">Setup</h3>

<p>As described above, high and low values for the 2nd principal component
appear to be related to a user’s tweeting about the Xbox or Hololens. As
such, I’ll select it for my predictive analysis. This can readily be
changed to another principal component, or the results of the sentiment
analysis, for other studies.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>possiblepreds &lt;- c('PC1','PC2','PC3','PC4','PC5')
choice &lt;- 'PC2'

# Remove columns
if(T){
    colremove &lt;- possiblepreds[sapply(possiblepreds, function(x) x!=choice)]
    col_list &lt;- colnames(alldata)
    final_cols &lt;- col_list[sapply(col_list, function(x) !(x %in% colremove))]
    df &lt;- alldata[final_cols]
}else{
    df &lt;- alldata
}

df &lt;- df %&gt;% 
    select(-user, -realname, -client) %&gt;%
    select(-(anger:trust)) # removing sentiment columns
</code></pre>
</div>

<p>In the event that there are parameters with little variance, I remove
them:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>nzv &lt;- nearZeroVar(df)
df_filter &lt;- df[, -nzv]
df_filter &lt;- na.omit(df_filter)
</code></pre>
</div>

<p>There are now only 9 parameters, including PC2, for our 7944 users.</p>

<p>I’ll now prepare my training and test data sets:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>set.seed(3456)
trainIndex &lt;- sample(nrow(df_filter), nrow(df_filter)*0.8)

df_train &lt;- df_filter[ trainIndex,]
df_test  &lt;- df_filter[-trainIndex,]
</code></pre>
</div>

<h3 id="regression-tree">Regression Tree</h3>

<p>First, I’ll run a regression tree model:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>rtGrid &lt;- expand.grid(cp=seq(0.005, 0.1, by = 0.005)) # grid of cp values
ctrl &lt;- trainControl(method = "cv", number = 10, verboseIter = F)

toRun &lt;- formula(paste0(choice,' ~ .'))
rtTune &lt;- train(toRun, data = df_train, method = "rpart", 
                tuneGrid = rtGrid, trControl = ctrl)
final_tree &lt;- rtTune$finalModel
</code></pre>
</div>

<p>The numeric values of our best fit are saved for later comparison:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>df_test_all &lt;- df_test
df_test_all[,'id'] &lt;- seq(nrow(df_test_all))
pr_rt &lt;- predict(rtTune, newdata = df_test)
rmseTree &lt;- RMSE(pr_rt, df_test[,choice])
modelSummary &lt;- data.frame(model='Regression Tree',
                         RMSE=rmseTree)

df_test_all[,'diff_Tree'] = df_test_all[,choice] - pr_rt
</code></pre>
</div>

<h3 id="generalized-linear-model">Generalized Linear Model</h3>

<p>I next run a generalized linear model:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>toRun &lt;- formula(paste0(choice,' ~ .'))
rtTune2 &lt;- train(toRun, data = df_train, method = "glm")

summary(rtTune2)
</code></pre>
</div>

<p>The summary suggests the most significant quantities to predict the
values of PC2 are the number of statuses (ie, tweets) and the positivity
value.</p>

<p>As before, we save the results for future comparison:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>pr_rt &lt;- predict(rtTune2, newdata = df_test)
rmseGLM &lt;- RMSE(pr_rt, df_test[,choice])
    
newRow &lt;- data.frame(model='Generalized LM',
                     RMSE=rmseGLM)
modelSummary &lt;- rbind(modelSummary, newRow)
    
df_test_all[,'diff_GLM'] = df_test_all[,choice] - pr_rt
</code></pre>
</div>

<h3 id="random-forest">Random Forest</h3>

<p>Next, I run a random forest model:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>rtGrid = expand.grid(mtry=seq(3, 7, by = 1)) # grid of mtry values
ctrl &lt;- trainControl(method = "cv", number = 5, verboseIter = F)

toRun &lt;- formula(paste0(choice,' ~ .'))
rtTune3 &lt;- train(toRun, data = df_train, 
                method = "rf",
                tuneGrid = rtGrid,
                trControl = ctrl, importance=T)
</code></pre>
</div>

<p>The best random forest model considers 3 random parameters at each
split. This is the overall importance of the various parameters:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>varImp(rtTune3)

## rf variable importance
## 
##                Overall
## positivity      100.00
## numlists         61.42
## followers        48.05
## numstatuses      41.74
## favorites        32.22
## twitter_years    22.83
## friends          22.06
## numTopicTweets    0.00

pr_rt &lt;- predict(rtTune3, newdata = df_test)
rmseRF &lt;- RMSE(pr_rt, df_test[,choice])
    
newRow &lt;- data.frame(model='Random Forest',
                     RMSE=rmseRF)
modelSummary &lt;- rbind(modelSummary, newRow)
    
df_test_all[,'diff_RF'] = df_test_all[,choice] - pr_rt
</code></pre>
</div>

<h3 id="glmnet">GLMNET</h3>

<p>Finally, I consider another generalized linear model with an elastic-net
penalty that controls whether we are considering a lasso or ridge
regression:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>enetGrid &lt;- expand.grid(.alpha = c(0, 0.1, 0.5, 0.7, 1), 
                        .lambda = seq(0, 5, by = 0.1))

ctrl &lt;- trainControl(method = "cv", number = 10, verboseIter = F)

toRun &lt;- formula(paste0(choice,' ~ .'))
rtTune4 &lt;- train(toRun, data = df_train,    
                  method = "glmnet", 
                  tuneGrid = enetGrid,
                  trControl = ctrl)
</code></pre>
</div>

<p>The best alpha and lambda parameters for the GLMNET model are 0, 0.1</p>

<div class="highlighter-rouge"><pre class="highlight"><code>pr_rt &lt;- predict(rtTune4, newdata = df_test)
rmseGLMNET &lt;- RMSE(pr_rt, df_test[,choice])
    
newRow &lt;- data.frame(model='GLMNET',
                     RMSE=rmseGLMNET)
modelSummary &lt;- rbind(modelSummary, newRow)
    
df_test_all[,'diff_GLMNET'] = df_test_all[,choice] - pr_rt
</code></pre>
</div>

<h2 id="model-comparison">Model Comparison</h2>

<p>The figure below compares the result of the various models:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>cols &lt;- c("RTree"="dark green", "GLM"="dark blue",
          "RForest"="dark orange", "GLMNET"="dark red")
alphalvl &lt;- 0.3
ggplot(data=df_test_all, aes(x=id)) +
    geom_point(aes(y=diff_Tree, color='RTree'), alpha=alphalvl) + 
    geom_hline(yintercept=c(rmseTree, -1*rmseTree), color=cols['RTree']) +
    geom_point(aes(y=diff_GLM, color='GLM'), alpha=alphalvl) + 
    geom_hline(yintercept=c(rmseGLM, -1*rmseGLM), color=cols['GLM']) +
    geom_point(aes(y=diff_RF, color='RForest'), alpha=alphalvl) + 
    geom_hline(yintercept=c(rmseRF, -1*rmseRF), color=cols['RForest']) +
    geom_point(aes(y=diff_GLMNET, color='GLMNET'), alpha=alphalvl) + 
    geom_hline(yintercept=c(rmseGLMNET, -1*rmseGLMNET), color=cols['GLMNET']) +
    theme_bw() + coord_cartesian(ylim=c(-2,2)) + 
    scale_colour_manual(name="Model",values=cols) +
    labs(x='Twitter User', y='Difference from Model')
</code></pre>
</div>

<p><img src="/ignited_projects/2016-04__David_Rodriguez/microsoft_analysis_files/figure-markdown_strict/rmse_plot-1.png" alt="" /></p>

<p>Here is a table of the root mean square errors (RMSE), a measure of the
goodness-of-fit:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>modelSummary %&gt;% kable(format='html')
</code></pre>
</div>

<table>
<thead>
<tr>
<th style="text-align:left;">
model
</th>
<th style="text-align:right;">
RMSE
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Regression Tree
</td>
<td style="text-align:right;">
0.7290474
</td>
</tr>
<tr>
<td style="text-align:left;">
Generalized LM
</td>
<td style="text-align:right;">
0.7281301
</td>
</tr>
<tr>
<td style="text-align:left;">
Random Forest
</td>
<td style="text-align:right;">
0.7065228
</td>
</tr>
<tr>
<td style="text-align:left;">
GLMNET
</td>
<td style="text-align:right;">
0.7284397
</td>
</tr>
</tbody>
</table>
<p>All models perform comparably similar with regards to the RMSE. As I
discuss below, there may be a few ways to improve the models in order to
yield more accurate results.</p>

<h2 id="summary">Summary</h2>

<p>I have looked at tweets concerning microsoft and have identified that
the second principal component (PC2) appears to be relevant to sorting
tweets by whether or not they are excited about the Hololens product or
Xbox games on their personal computers. While in principle, there are a
lot of tweets and variations, I chose to use PC2 as a measure of the
phrases used and developed a set of models to attempt to predict what
values of PC2 a user would have.</p>

<p>This approach can be useful to determine what types of users are excited
about the Hololens and which are excited about Xbox games (in this
example) and can suggest targetted advertizing.</p>

<p>Let’s have a look at the regression tree as it’s one of the clearest to
describe:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>fancyRpartPlot(final_tree, palettes=c("Blues"), sub='')
</code></pre>
</div>

<p><img src="/ignited_projects/2016-04__David_Rodriguez/microsoft_analysis_files/figure-markdown_strict/rt_plot-1.png" alt="" /></p>

<p>The above suggests that users with very negative PC2 values (associated
with excitement about the Xbox product) also have very few followers and
have been on Twitter a very small amount of time. This suggest these
types of users are actually Twitter robot accounts created to spam
advertisement on this particular Xbox news story. I would argue it’s
safe to disregard spending any advertising efforts on these users.</p>

<p>On the other hand, for higher PC2 values (associated with excitement
about the Hololens product), we can see more meaningful information. The
value of PC2 for a user depends on the postivity, which is a measure on
how often positive and negative words are used; the number of statuses
or tweets they’ve had; the number of lists they follow; and the number
of Twitter favorites they have.</p>

<p>While these models can predict the values of PC2, their errors remain
fairly large. I interpret this as the PC2 value having large variation
in terms of the word choices used to construct the individual tweets. A
possible way to improve these models would be to consider more
parameters given that we have enough data to support this. These
additional parameters could come from twitter or from external sources.
Another possibility is to re-examine our source of data. Rather than
gathering ‘recent’ tweets, we could have gathered ‘popuplar’ or ‘mixed’
tweets, which would rely on Twitter’s algorithms to return a different
sample of tweets. Yet another possibility would be to consider a
different API, such as gathering data from Facebook.</p>

<h2 id="conclusions">Conclusions</h2>

<p>In the end, while this project demonstrated a potential relationship
between word choice and differentiation between Microsoft products, it
can readily be expanded to any other topic of interest. However, given
the varied nature of tweeted topics within a search, it is not always
clear that a trend can or will emerge.</p>

<p>Alternative ways in which Twitter data could be used include:</p>

<ul>
  <li>Examining trends of stock market prices with respect to word choices
or overall sentiment in Twitter</li>
  <li>Determining what properties of a tweet or user make for the most
popular tweets in order to best market your product</li>
  <li>Comparing two search terms for two separate companies to identify
and address any issues or complaints</li>
  <li>Exploring tweets by location to find popular venues or events</li>
</ul>

  </article>

</div>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

<!--     <h2 class="footer-heading">Ket Labs</h2> -->

    <div class="footer-col-wrapper">
      <div class="footer-col  footer-col-1">
        <ul class="contact-list">
          <li><strong>Ket Labs</strong></li>

          <li><a href="mailto:ashehu@ket-labs.com">ashehu@ket-labs.com</a></li>
        </ul>
      </div>

      <div class="footer-col  footer-col-2">
        <ul class="social-media-list">
          

          
        </ul>
      </div>

      <div class="footer-col  footer-col-3">
         <p class="text">
Get a data science upgrade for your team, research lab or personal project. 
 
      </div>
    </div>

  </div>

</footer>

  </body>

</html>
